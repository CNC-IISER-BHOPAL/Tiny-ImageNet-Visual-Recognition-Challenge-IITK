{"cells":[{"metadata":{},"cell_type":"markdown","source":["# Transfer Learning for Image Classification, by Ayush Dabra, IISER Bhopal\n"," **Brief Overview-**\n","* The dataset contains square images of 64x64 pixels.\n","* Each image belongs to exactly one out of 200 categories.\n","* The training set contains 90,000 images, 450 from each category.\n","* The validation and test sets have 10,000 images each, 50 from each category.\n","* Modified VGG16 model is used for training.\n","* Heavy image augmentation techniques are used for avoiding overfitting and better accuracy."]},{"metadata":{},"cell_type":"markdown","source":["# Installing and Importing Libraries\n","* Tensorflow and Keras frameworks are used for this project.\n","* Keras-Contrib library is for CyclicLR callback.\n","* Keract library is for visualizing the layer outputs."]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":false},"cell_type":"code","source":["!pip install git+https://github.com/keras-team/keras-contrib.git\n","!pip install keract"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from numpy import expand_dims\n","import matplotlib.pyplot as plt\n","import keract\n","from tensorflow import keras\n","from tensorflow.keras import applications\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n","from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n","from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","from keras_contrib.callbacks import CyclicLR"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Dataset"]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["train_data_dir = '../input/imagedetect/train'  \n","validation_data_dir = '../input/imagedetect/val' \n","test_data_dir = '../input/imagedetect/test'"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["img_width, img_height = 64, 64 \n","channels = 3\n","batch_size = 64"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["val_data = pd.read_csv(validation_data_dir + '/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n","val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n","val_data.head(3)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["train_datagen = ImageDataGenerator(\n","    rescale= 1./255,\n","    shear_range= 0.2,\n","    zoom_range= 0.2,\n","    horizontal_flip= True,\n","    rotation_range= 20,\n","    width_shift_range= 0.2,\n","    height_shift_range= 0.2,\n",")\n","\n","datagen = ImageDataGenerator(rescale= 1./255)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["train_generator = train_datagen.flow_from_directory(  \n","    train_data_dir,  \n","    target_size= (img_width, img_height), \n","    color_mode= 'rgb',\n","    batch_size= batch_size,  \n","    class_mode= 'categorical',\n","    shuffle= True, \n","    seed= 42\n",") \n","\n","valid_generator = datagen.flow_from_dataframe(\n","    dataframe= val_data, \n","    directory= validation_data_dir + '/images',\n","    x_col= 'File', \n","    y_col= 'Class', \n","    target_size= (img_width, img_height),\n","    color_mode= 'rgb', \n","    class_mode= 'categorical', \n","    batch_size= batch_size, \n","    shuffle= True, \n","    seed= 42\n",")\n","\n","test_generator = datagen.flow_from_directory(  \n","    test_data_dir,  \n","    target_size= (img_width, img_height), \n","    color_mode= 'rgb',\n","    batch_size= batch_size,\n","    class_mode= None,\n","    shuffle= False, \n",")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["num_classes = len(train_generator.class_indices)  \n","train_labels = train_generator.classes \n","train_labels = to_categorical(train_labels, num_classes=num_classes)\n","valid_labels = valid_generator.classes \n","valid_labels = to_categorical(valid_labels, num_classes=num_classes)\n","nb_train_samples = len(train_generator.filenames)  \n","nb_valid_samples = len(valid_generator.filenames)\n","nb_test_samples = len(test_generator.filenames)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["img = load_img('../input/imagedetect/train/n02279972/images/n02279972_28.JPEG')\n","data = img_to_array(img)\n","samples = expand_dims(data, 0)\n","it = train_datagen.flow(samples, batch_size=1)\n","\n","for i in range(9):\n","\tplt.subplot(330 + 1 + i)\n","\tbatch = it.next()\n","\timage = batch[0]\n","\tplt.imshow(image)\n","\n","plt.savefig('augmented_image.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n","plt.show()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# **Model**\n","* Modified VGG16 CNN architecture is used for the problem.\n","* Pretrained on the 'ImageNet' dataset."]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["vgg16 = applications.VGG16(include_top= False, input_shape= (img_width, img_height, channels), weights= 'imagenet')\n","vgg16.summary()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["model = Sequential()\n","\n","for layer in vgg16.layers:\n","    model.add(layer)\n","\n","for layer in model.layers:\n","    layer.trainable= False\n","\n","model.add(Flatten(input_shape= (2, 2, 512)))\n","\n","model.add(Dense(512, activation= 'relu', name= 'FC1'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.4))\n","\n","model.add(Dense(512, activation= 'relu', name= 'FC2'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.4))\n","\n","model.add(Dense(200, activation= 'softmax', name= 'FC3'))\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Baseline Model Training\n","* Cyclical LR in range of 1e-4 to 6e-4 with a step size of 1404."]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["model.compile(optimizer= keras.optimizers.Adam(lr= 0.0001, epsilon= 1e-08), loss= 'categorical_crossentropy', metrics= ['accuracy'])"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["my_callbacks = [\n","    EarlyStopping(monitor= 'val_accuracy', mode= 'auto', patience=2),\n","    CyclicLR(base_lr=0.0001, max_lr=0.0006, step_size=1404., mode= 'triangular2'),\n","    ModelCheckpoint(filepath= 'baseline_model.h5', monitor= 'val_accuracy', save_best_only= True, mode= 'auto')\n","]"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["history = model.fit(\n","    train_generator, \n","    epochs= 30,\n","    steps_per_epoch = nb_train_samples//batch_size, \n","    validation_data = valid_generator, \n","    validation_steps = nb_valid_samples//batch_size,\n","    verbose = 2, \n","    callbacks = my_callbacks,\n","    shuffle = True\n",")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["(eval_loss, eval_accuracy) = model.evaluate(valid_generator, batch_size= batch_size, verbose= 1)\n","print('Validation Loss: ', eval_loss)\n","print('Validation Accuracy: ', eval_accuracy)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Training Accuracy','Validation Accuracy'])\n","plt.savefig('base_acc_epoch.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n","plt.show()\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training Loss','Validation Loss'])\n","plt.savefig('base_loss_epoch.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n","plt.show()\n","\n","plt.plot(history.history['lr'])\n","plt.ylabel('Learning Rate')\n","plt.xlabel('Epoch')\n","plt.legend(['Learning Rate'])\n","plt.savefig('base_lr_epoch.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n","plt.show()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Finetuning\n","* Cyclical LR in range of 1e-5 to 6e-5 with a setp size of 1200.\n","* Further, reducing cyclical LR in the range of 6e-6 to 6e-6, keeping the step size same."]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["model.trainable= True\n","model.compile(optimizer= keras.optimizers.Adam(1e-5), loss= 'categorical_crossentropy', metrics= ['accuracy'])"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["my_callbacks = [\n","    CyclicLR(base_lr= 0.00001, max_lr= 0.00006, step_size= 1200., mode= 'triangular2'),\n","    ModelCheckpoint(filepath= 'finetuned_model_v1.h5', monitor= 'val_accuracy', save_best_only= True, mode= 'auto')\n","]"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["history_1= model.fit(\n","    train_generator, \n","    epochs= 15, \n","    steps_per_epoch= nb_train_samples//batch_size, \n","    validation_data= valid_generator, \n","    validation_steps= nb_valid_samples//batch_size,\n","    verbose= 2, \n","    callbacks= my_callbacks\n",")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["(eval_loss, eval_accuracy) = model.evaluate(valid_generator, batch_size= batch_size, verbose= 1)\n","print('Validation Loss: ', eval_loss)\n","print('Validation Accuracy: ', eval_accuracy)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["plt.plot(history_1.history['accuracy'])\n","plt.plot(history_1.history['val_accuracy'])\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Training Accuracy','Validation Accuracy'])\n","plt.savefig('finetuned_acc_epoch_v1.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n","plt.show()\n","\n","plt.plot(history_1.history['loss'])\n","plt.plot(history_1.history['val_loss'])\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training Loss','Validation Loss'])\n","plt.savefig('finetuned_loss_epoch_v1.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n","plt.show()\n","\n","plt.plot(history_1.history['lr'])\n","plt.ylabel('Learning Rate')\n","plt.xlabel('Epoch')\n","plt.legend(['Learning Rate'])\n","plt.savefig('finetuned_lr_epoch_v1.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n","plt.show()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["model.compile(optimizer= keras.optimizers.Adam(1e-6), loss= 'categorical_crossentropy', metrics= ['accuracy'])"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["my_callbacks = [\n","    CyclicLR(base_lr=0.000001, max_lr=0.000006, step_size=1200., mode= 'triangular2'),\n","    ModelCheckpoint(filepath= 'finetuned_model_v2.h5', monitor= 'val_accuracy', save_best_only= True, mode= 'auto')\n","]"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["history_2 = model.fit(\n","    train_generator, \n","    epochs= 15, \n","    steps_per_epoch= nb_train_samples//batch_size, \n","    validation_data= valid_generator, \n","    validation_steps= nb_valid_samples//batch_size,\n","    verbose= 2, \n","    callbacks= my_callbacks\n",")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["(eval_loss, eval_accuracy) = model.evaluate(valid_generator, batch_size= batch_size, verbose= 1)\n","print('Validation Loss: ', eval_loss)\n","print('Validation Accuracy: ', eval_accuracy)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["plt.plot(history_2.history['accuracy'])\n","plt.plot(history_2.history['val_accuracy'])\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Training Accuracy','Validation Accuracy'])\n","plt.savefig('finetuned_acc_epoch_v2.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n","plt.show()\n","\n","plt.plot(history_2.history['loss'])\n","plt.plot(history_2.history['val_loss'])\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training Loss','Validation Loss'])\n","plt.savefig('finetuned_loss_epoch_v2.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n","plt.show()\n","\n","plt.plot(history_2.history['lr'])\n","plt.ylabel('Learning Rate')\n","plt.xlabel('Epoch')\n","plt.legend(['Learning Rate'])\n","plt.savefig('finetuned_lr_epoch_v2.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n","plt.show()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Predictions on Test Set"]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["def getListKeys(dict): \n","    list = [] \n","    for key in dict.keys(): \n","        list.append(key) \n","    return np.asarray(list)\n","\n","labels = getListKeys(train_generator.class_indices)\n","\n","filenames = np.array([])\n","file_names = test_generator.filenames\n","for file in file_names:\n","    temp = file.split('/')\n","    filenames = np.append(filenames, temp[1])"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["predict = model.predict(test_generator)\n","predicted_class_indices= np.argmax(predict,axis=1)\n","predictions= np.asarray([labels[i] for i in predicted_class_indices])\n","\n","test_df = pd.DataFrame()\n","test_df['file_name'] = filenames\n","test_df['category'] = predictions\n","test_df.to_csv('test_predictions.csv', index= False)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# **Visualizing Layers**"]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["image = load_img('../input/imagedetect/train/n02124075/images/n02124075_360.JPEG', target_size= (64, 64))\n","image = img_to_array(image)\n","image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","image = preprocess_input(image)\n","y_hat = model.predict(image)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":["activations= keract.get_activations(model, image, layer_names= None, nodes_to_evaluate= None, output_format= 'simple', auto_compile= True)\n","keract.display_activations(activations, save= False)"],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}