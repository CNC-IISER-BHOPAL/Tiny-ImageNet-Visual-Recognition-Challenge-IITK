{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom VGG16 Model for Tiny ImageNet Classification\n",
    " **Brief Overview-**\n",
    "* The dataset contains square images of 64x64 pixels.\n",
    "* Each image belongs to exactly one out of 200 categories.\n",
    "* The training set contains 90,000 images, 450 from each category.\n",
    "* The validation and test sets have 10,000 images each, 50 from each category.\n",
    "* Modified VGG16 model is used for training.\n",
    "* Image augmentation techniques are used for avoiding overfitting and better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing and Importing Libraries\n",
    "* Tensorflow and Keras frameworks are used for this project.\n",
    "* Keras-Contrib library is for CyclicLR callback.\n",
    "* Keract library is for visualizing the layer outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/keras-team/keras-contrib.git\n",
    "!pip install keract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import expand_dims\n",
    "import matplotlib.pyplot as plt\n",
    "import keract\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras_contrib.callbacks import CyclicLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = '../input/imagedetect/train'  \n",
    "validation_data_dir = '../input/imagedetect/val' \n",
    "test_data_dir = '../input/imagedetect/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 64, 64 \n",
    "channels = 3\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_csv(validation_data_dir + '/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
    "val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n",
    "val_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale= 1./255,\n",
    "    shear_range= 0.2,\n",
    "    zoom_range= 0.2,\n",
    "    horizontal_flip= True,\n",
    "    rotation_range= 20,\n",
    "    width_shift_range= 0.2,\n",
    "    height_shift_range= 0.2,\n",
    ")\n",
    "\n",
    "datagen = ImageDataGenerator(rescale= 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(  \n",
    "    train_data_dir,  \n",
    "    target_size= (img_width, img_height), \n",
    "    color_mode= 'rgb',\n",
    "    batch_size= batch_size,  \n",
    "    class_mode= 'categorical',\n",
    "    shuffle= True, \n",
    "    seed= 42\n",
    ") \n",
    "\n",
    "valid_generator = datagen.flow_from_dataframe(\n",
    "    dataframe= val_data, \n",
    "    directory= validation_data_dir + '/images',\n",
    "    x_col= 'File', \n",
    "    y_col= 'Class', \n",
    "    target_size= (img_width, img_height),\n",
    "    color_mode= 'rgb', \n",
    "    class_mode= 'categorical', \n",
    "    batch_size= batch_size, \n",
    "    shuffle= True, \n",
    "    seed= 42\n",
    ")\n",
    "\n",
    "test_generator = datagen.flow_from_directory(  \n",
    "    test_data_dir,  \n",
    "    target_size= (img_width, img_height), \n",
    "    color_mode= 'rgb',\n",
    "    batch_size= batch_size,\n",
    "    class_mode= None,\n",
    "    shuffle= False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_generator.class_indices)  \n",
    "train_labels = train_generator.classes \n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
    "valid_labels = valid_generator.classes \n",
    "valid_labels = to_categorical(valid_labels, num_classes=num_classes)\n",
    "nb_train_samples = len(train_generator.filenames)  \n",
    "nb_valid_samples = len(valid_generator.filenames)\n",
    "nb_test_samples = len(test_generator.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img('../input/imagedetect/train/n02279972/images/n02279972_28.JPEG')\n",
    "data = img_to_array(img)\n",
    "samples = expand_dims(data, 0)\n",
    "it = train_datagen.flow(samples, batch_size=1)\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    batch = it.next()\n",
    "    image = batch[0]\n",
    "    plt.imshow(image)\n",
    "\n",
    "plt.savefig('augmented_image.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "* Modified VGG16 CNN architecture is used for the problem.\n",
    "* Pretrained on the 'ImageNet' dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = applications.VGG16(include_top= False, input_shape= (img_width, img_height, channels), weights= 'imagenet')\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "for layer in vgg16.layers:\n",
    "    model.add(layer)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable= False\n",
    "\n",
    "model.add(Flatten(input_shape= (2, 2, 512)))\n",
    "\n",
    "model.add(Dense(512, activation= 'relu', name= 'FC1'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(512, activation= 'relu', name= 'FC2'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(200, activation= 'softmax', name= 'FC3'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Training\n",
    "* Cyclical LR in range of 1e-4 to 6e-4 with a step size of 1404."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= keras.optimizers.Adam(lr= 0.0001, epsilon= 1e-08), loss= 'categorical_crossentropy', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "    EarlyStopping(monitor= 'val_accuracy', mode= 'auto', patience=2),\n",
    "    CyclicLR(base_lr=0.0001, max_lr=0.0006, step_size=1404., mode= 'triangular2'),\n",
    "    ModelCheckpoint(filepath= 'baseline_model.h5', monitor= 'val_accuracy', save_best_only= True, mode= 'auto')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator, \n",
    "    epochs= 30,\n",
    "    steps_per_epoch = nb_train_samples//batch_size, \n",
    "    validation_data = valid_generator, \n",
    "    validation_steps = nb_valid_samples//batch_size,\n",
    "    verbose = 2, \n",
    "    callbacks = my_callbacks,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(eval_loss, eval_accuracy) = model.evaluate(valid_generator, batch_size= batch_size, verbose= 1)\n",
    "print('Validation Loss: ', eval_loss)\n",
    "print('Validation Accuracy: ', eval_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Accuracy','Validation Accuracy'])\n",
    "plt.savefig('base_acc_epoch.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Loss','Validation Loss'])\n",
    "plt.savefig('base_loss_epoch.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['lr'])\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Learning Rate'])\n",
    "plt.savefig('base_lr_epoch.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning\n",
    "* Cyclical LR in range of 1e-5 to 6e-5 with a setp size of 1200.\n",
    "* Further, reducing cyclical LR in the range of 6e-6 to 6e-6, keeping the step size same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable= True\n",
    "model.compile(optimizer= keras.optimizers.Adam(1e-5), loss= 'categorical_crossentropy', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "    CyclicLR(base_lr= 0.00001, max_lr= 0.00006, step_size= 1200., mode= 'triangular2'),\n",
    "    ModelCheckpoint(filepath= 'finetuned_model_v1.h5', monitor= 'val_accuracy', save_best_only= True, mode= 'auto')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_1= model.fit(\n",
    "    train_generator, \n",
    "    epochs= 15, \n",
    "    steps_per_epoch= nb_train_samples//batch_size, \n",
    "    validation_data= valid_generator, \n",
    "    validation_steps= nb_valid_samples//batch_size,\n",
    "    verbose= 2, \n",
    "    callbacks= my_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(eval_loss, eval_accuracy) = model.evaluate(valid_generator, batch_size= batch_size, verbose= 1)\n",
    "print('Validation Loss: ', eval_loss)\n",
    "print('Validation Accuracy: ', eval_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_1.history['accuracy'])\n",
    "plt.plot(history_1.history['val_accuracy'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Accuracy','Validation Accuracy'])\n",
    "plt.savefig('finetune_acc_epoch_v1.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_1.history['loss'])\n",
    "plt.plot(history_1.history['val_loss'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Loss','Validation Loss'])\n",
    "plt.savefig('finetuned_loss_epoch_v1.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_1.history['lr'])\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Learning Rate'])\n",
    "plt.savefig('finetuned_lr_epoch_v1.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= keras.optimizers.Adam(1e-6), loss= 'categorical_crossentropy', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "    CyclicLR(base_lr=0.000001, max_lr=0.000006, step_size=1200., mode= 'triangular2'),\n",
    "    ModelCheckpoint(filepath= 'finetuned_model_v2.h5', monitor= 'val_accuracy', save_best_only= True, mode= 'auto')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_2 = model.fit(\n",
    "    train_generator, \n",
    "    epochs= 15, \n",
    "    steps_per_epoch= nb_train_samples//batch_size, \n",
    "    validation_data= valid_generator, \n",
    "    validation_steps= nb_valid_samples//batch_size,\n",
    "    verbose= 2, \n",
    "    callbacks= my_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(eval_loss, eval_accuracy) = model.evaluate(valid_generator, batch_size= batch_size, verbose= 1)\n",
    "print('Validation Loss: ', eval_loss)\n",
    "print('Validation Accuracy: ', eval_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_2.history['accuracy'])\n",
    "plt.plot(history_2.history['val_accuracy'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Accuracy','Validation Accuracy'])\n",
    "plt.savefig('finetune_acc_epoch_v2.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_2.history['loss'])\n",
    "plt.plot(history_2.history['val_loss'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Loss','Validation Loss'])\n",
    "plt.savefig('finetuned_loss_epoch_v2.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_2.history['lr'])\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Learning Rate'])\n",
    "plt.savefig('finetuned_lr_epoch_v2.png', transparent= True, bbox_inches= 'tight', dpi= 900)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListKeys(dict): \n",
    "    list = [] \n",
    "    for key in dict.keys(): \n",
    "        list.append(key) \n",
    "    return np.asarray(list)\n",
    "\n",
    "labels = getListKeys(train_generator.class_indices)\n",
    "\n",
    "filenames = np.array([])\n",
    "file_names = test_generator.filenames\n",
    "for file in file_names:\n",
    "    temp = file.split('/')\n",
    "    filenames = np.append(filenames, temp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(test_generator)\n",
    "predicted_class_indices= np.argmax(predict,axis=1)\n",
    "predictions= np.asarray([labels[i] for i in predicted_class_indices])\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "test_df['file_name'] = filenames\n",
    "test_df['category'] = predictions\n",
    "test_df.to_csv('test_predictions.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_img('../input/imagedetect/train/n02124075/images/n02124075_360.JPEG', target_size= (64, 64))\n",
    "image = img_to_array(image)\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "image = preprocess_input(image)\n",
    "y_hat = model.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations= keract.get_activations(model, image, layer_names= None, nodes_to_evaluate= None, output_format= 'simple', auto_compile= True)\n",
    "keract.display_activations(activations, save= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
