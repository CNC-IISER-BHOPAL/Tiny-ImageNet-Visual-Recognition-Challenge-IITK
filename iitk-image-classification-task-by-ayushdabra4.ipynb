{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Transfer Learning for Image Classification, by Ayush Dabra, IISER Bhopal\n **Brief Overview-**\n* The dataset contains square images of 64x64 pixels.\n* Each image belongs to exactly one out of 200 categories.\n* The training set contains 90,000 images, 450 from each category.\n* The validation and test sets have 10,000 images each, 50 from each category.\n* Modified VGG16 model is used for training.\n* Image augmentation techniques are used for avoiding overfitting and better accuracy.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Installing and Importing Libraries\n* Tensorflow and Keras frameworks are used for this project.\n* Keras-Contrib library is for CyclicLR callback.\n* Keract library is for visualizing the layer outputs.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":false},"cell_type":"code","source":"!pip install git+https://github.com/keras-team/keras-contrib.git\n!pip install keract","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom numpy import expand_dims\nimport matplotlib.pyplot as plt\nimport keract\nfrom tensorflow import keras\nfrom tensorflow.keras import applications\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras_contrib.callbacks import CyclicLR","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_data_dir = '../input/imagedetect/train'  \nvalidation_data_dir = '../input/imagedetect/val' \ntest_data_dir = '../input/imagedetect/test'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"img_width, img_height = 64, 64 \nchannels = 3\nbatch_size = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"val_data = pd.read_csv(validation_data_dir + '/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\nval_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\nval_data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n    rescale= 1./255,\n    shear_range= 0.2,\n    zoom_range= 0.2,\n    horizontal_flip= True,\n    rotation_range= 20,\n    width_shift_range= 0.2,\n    height_shift_range= 0.2,\n)\n\ndatagen = ImageDataGenerator(rescale= 1./255)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_generator = train_datagen.flow_from_directory(  \n    train_data_dir,  \n    target_size= (img_width, img_height), \n    color_mode= 'rgb',\n    batch_size= batch_size,  \n    class_mode= 'categorical',\n    shuffle= True, \n    seed= 42\n) \n\nvalid_generator = datagen.flow_from_dataframe(\n    dataframe= val_data, \n    directory= validation_data_dir + '/images',\n    x_col= 'File', \n    y_col= 'Class', \n    target_size= (img_width, img_height),\n    color_mode= 'rgb', \n    class_mode= 'categorical', \n    batch_size= batch_size, \n    shuffle= True, \n    seed= 42\n)\n\ntest_generator = datagen.flow_from_directory(  \n    test_data_dir,  \n    target_size= (img_width, img_height), \n    color_mode= 'rgb',\n    batch_size= batch_size,\n    class_mode= None,\n    shuffle= False, \n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"num_classes = len(train_generator.class_indices)  \ntrain_labels = train_generator.classes \ntrain_labels = to_categorical(train_labels, num_classes=num_classes)\nvalid_labels = valid_generator.classes \nvalid_labels = to_categorical(valid_labels, num_classes=num_classes)\nnb_train_samples = len(train_generator.filenames)  \nnb_valid_samples = len(valid_generator.filenames)\nnb_test_samples = len(test_generator.filenames)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"img = load_img('../input/imagedetect/train/n02279972/images/n02279972_28.JPEG')\ndata = img_to_array(img)\nsamples = expand_dims(data, 0)\nit = train_datagen.flow(samples, batch_size=1)\n\nfor i in range(9):\n\tplt.subplot(330 + 1 + i)\n\tbatch = it.next()\n\timage = batch[0]\n\tplt.imshow(image)\n\nplt.savefig('augmented_image.png', transparent= True, bbox_inches= 'tight', dpi= 900)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model**\n* Modified VGG16 CNN architecture is used for the problem.\n* Pretrained on the 'ImageNet' dataset.","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"vgg16 = applications.VGG16(include_top= False, input_shape= (img_width, img_height, channels), weights= 'imagenet')\nvgg16.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"model = Sequential()\n\nfor layer in vgg16.layers:\n    model.add(layer)\n\nfor layer in model.layers:\n    layer.trainable= False\n\nmodel.add(Flatten(input_shape= (2, 2, 512)))\n\nmodel.add(Dense(512, activation= 'relu', name= 'FC1'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(512, activation= 'relu', name= 'FC2'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(200, activation= 'softmax', name= 'FC3'))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline Model Training\n* Cyclical LR in range of 1e-4 to 6e-4 with a step size of 1404.","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"model.compile(optimizer= keras.optimizers.Adam(lr= 0.0001, epsilon= 1e-08), loss= 'categorical_crossentropy', metrics= ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"my_callbacks = [\n    EarlyStopping(monitor= 'val_accuracy', mode= 'auto', patience=2),\n    CyclicLR(base_lr=0.0001, max_lr=0.0006, step_size=1404., mode= 'triangular2'),\n    ModelCheckpoint(filepath= 'baseline_model.h5', monitor= 'val_accuracy', save_best_only= True, mode= 'auto')\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"history = model.fit(\n    train_generator, \n    epochs= 30,\n    steps_per_epoch = nb_train_samples//batch_size, \n    validation_data = valid_generator, \n    validation_steps = nb_valid_samples//batch_size,\n    verbose = 2, \n    callbacks = my_callbacks,\n    shuffle = True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"(eval_loss, eval_accuracy) = model.evaluate(valid_generator, batch_size= batch_size, verbose= 1)\nprint('Validation Loss: ', eval_loss)\nprint('Validation Accuracy: ', eval_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Training Accuracy','Validation Accuracy'])\nplt.savefig('base_acc_epoch.png', transparent= True, bbox_inches= 'tight', dpi= 900)\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training Loss','Validation Loss'])\nplt.savefig('base_loss_epoch.png', transparent= True, bbox_inches= 'tight', dpi= 900)\nplt.show()\n\nplt.plot(history.history['lr'])\nplt.ylabel('Learning Rate')\nplt.xlabel('Epoch')\nplt.legend(['Learning Rate'])\nplt.savefig('base_lr_epoch.png', transparent= True, bbox_inches= 'tight', dpi= 900)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finetuning\n* Cyclical LR in range of 1e-5 to 6e-5 with a setp size of 1200.\n* Further, reducing cyclical LR in the range of 6e-6 to 6e-6, keeping the step size same.","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"model.trainable= True\nmodel.compile(optimizer= keras.optimizers.Adam(1e-5), loss= 'categorical_crossentropy', metrics= ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"my_callbacks = [\n    CyclicLR(base_lr= 0.00001, max_lr= 0.00006, step_size= 1200., mode= 'triangular2'),\n    ModelCheckpoint(filepath= 'finetuned_model_v1.h5', monitor= 'val_accuracy', save_best_only= True, mode= 'auto')\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"history_1= model.fit(\n    train_generator, \n    epochs= 15, \n    steps_per_epoch= nb_train_samples//batch_size, \n    validation_data= valid_generator, \n    validation_steps= nb_valid_samples//batch_size,\n    verbose= 2, \n    callbacks= my_callbacks\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"(eval_loss, eval_accuracy) = model.evaluate(valid_generator, batch_size= batch_size, verbose= 1)\nprint('Validation Loss: ', eval_loss)\nprint('Validation Accuracy: ', eval_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.plot(history_1.history['accuracy'])\nplt.plot(history_1.history['val_accuracy'])\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Training Accuracy','Validation Accuracy'])\nplt.savefig('finetune_acc_epoch_v1.png', transparent= True, bbox_inches= 'tight', dpi= 900)\nplt.show()\n\nplt.plot(history_1.history['loss'])\nplt.plot(history_1.history['val_loss'])\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training Loss','Validation Loss'])\nplt.savefig('finetuned_loss_epoch_v1.png', transparent= True, bbox_inches= 'tight', dpi= 900)\nplt.show()\n\nplt.plot(history_1.history['lr'])\nplt.ylabel('Learning Rate')\nplt.xlabel('Epoch')\nplt.legend(['Learning Rate'])\nplt.savefig('finetuned_lr_epoch_v1.png', transparent= True, bbox_inches= 'tight', dpi= 900)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"model.compile(optimizer= keras.optimizers.Adam(1e-6), loss= 'categorical_crossentropy', metrics= ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"my_callbacks = [\n    CyclicLR(base_lr=0.000001, max_lr=0.000006, step_size=1200., mode= 'triangular2'),\n    ModelCheckpoint(filepath= 'finetuned_model_v2.h5', monitor= 'val_accuracy', save_best_only= True, mode= 'auto')\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"history_2 = model.fit(\n    train_generator, \n    epochs= 15, \n    steps_per_epoch= nb_train_samples//batch_size, \n    validation_data= valid_generator, \n    validation_steps= nb_valid_samples//batch_size,\n    verbose= 2, \n    callbacks= my_callbacks\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"(eval_loss, eval_accuracy) = model.evaluate(valid_generator, batch_size= batch_size, verbose= 1)\nprint('Validation Loss: ', eval_loss)\nprint('Validation Accuracy: ', eval_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.plot(history_2.history['accuracy'])\nplt.plot(history_2.history['val_accuracy'])\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Training Accuracy','Validation Accuracy'])\nplt.savefig('finetune_acc_epoch_v2.png', transparent= True, bbox_inches= 'tight', dpi= 900)\nplt.show()\n\nplt.plot(history_2.history['loss'])\nplt.plot(history_2.history['val_loss'])\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training Loss','Validation Loss'])\nplt.savefig('finetuned_loss_epoch_v2.png', transparent= True, bbox_inches= 'tight', dpi= 900)\nplt.show()\n\nplt.plot(history_2.history['lr'])\nplt.ylabel('Learning Rate')\nplt.xlabel('Epoch')\nplt.legend(['Learning Rate'])\nplt.savefig('finetuned_lr_epoch_v2.png', transparent= True, bbox_inches= 'tight', dpi= 900)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions on Test Set","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def getListKeys(dict): \n    list = [] \n    for key in dict.keys(): \n        list.append(key) \n    return np.asarray(list)\n\nlabels = getListKeys(train_generator.class_indices)\n\nfilenames = np.array([])\nfile_names = test_generator.filenames\nfor file in file_names:\n    temp = file.split('/')\n    filenames = np.append(filenames, temp[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"predict = model.predict(test_generator)\npredicted_class_indices= np.argmax(predict,axis=1)\npredictions= np.asarray([labels[i] for i in predicted_class_indices])\n\ntest_df = pd.DataFrame()\ntest_df['file_name'] = filenames\ntest_df['category'] = predictions\ntest_df.to_csv('test_predictions.csv', index= False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Visualizing Layers**","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"image = load_img('../input/imagedetect/train/n02124075/images/n02124075_360.JPEG', target_size= (64, 64))\nimage = img_to_array(image)\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\nimage = preprocess_input(image)\ny_hat = model.predict(image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"activations= keract.get_activations(model, image, layer_names= None, nodes_to_evaluate= None, output_format= 'simple', auto_compile= True)\nkeract.display_activations(activations, save= False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}